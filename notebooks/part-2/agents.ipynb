{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c804c08-8dd6-43c1-abbb-737e296535f8",
   "metadata": {},
   "source": [
    "# Working with Agents\n",
    "\n",
    "An agent is a language model that decides on what actions to take for a given prompt and in which order.\n",
    "\n",
    "In today's example, we are going to an agent that decides whether to retrieve information from an LLM's general knowledge (by passing a prompt to the underlying LLM) or by calling an API.\n",
    "\n",
    "## The innate staleness of LLM data\n",
    "\n",
    "One of the downsides of LLMs is that their training data only captures a snapshot of human knowledge up to a certain point in time. This cutoff creates an inherent staleness in the data, as the model cannot reflect information beyond its last update. For instance, let's try asking an LLM what the current weather in New York is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0debd3-8760-4ff1-8656-1c8ed62f8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Context window of mistral is rougly 8k\n",
    "# https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html#langchain_community.llms.ollama.Ollama\n",
    "llm = ChatOllama(model=\"llama3.1:8b\",\n",
    "            # temperature=0.9, # Increasing the temperature will make the model answer more creatively. (Default: 0.8),\n",
    "            # num_ctx=4096  # Default is 2048\n",
    "            )\n",
    "\n",
    "llm.invoke(\"Please tell me the current weather in New York city for today\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2bc13-ac88-4d86-957e-1f051fb3094d",
   "metadata": {},
   "source": [
    "Clearly this demonstrates the above point regarding the cutoff. There is also no built-in way to check online weather sources. Repeating the above query can yield different results too - with a high likelyhood of hallucinations. This is where Agents can help us when it comes to using alternate sources of information - for instance the current weather in a certain location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7bcc5e-9c8f-4627-818b-c836e5bdad53",
   "metadata": {},
   "source": [
    "## API setup\n",
    "\n",
    "Let us first create a function that performs an API call to the weatherapi, which provides realtime weather data for a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d6d6f-ecc0-486c-b3c8-0e600c1c556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Note: for demo purposes - throwaway key. In practice you should NEVER expose API keys like this (use ie env variables)\n",
    "FREE_WEATHER_API_KEY=\"aa96b0814fc94f09847100745241005\"\n",
    "\n",
    "def make_weather_api_call(location: str):\n",
    "    response = requests.get(f\"https://api.weatherapi.com/v1/current.json?key={FREE_WEATHER_API_KEY}&q={location}\")\n",
    "    if (response.status_code != 200):\n",
    "        raise ValueError(\"Could not obtain the weather for the current parameters\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb84dbc-2ff7-430e-9236-c6605f67b556",
   "metadata": {},
   "source": [
    "Let's try this out to get the current London weather and inspect the resultant object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f5373-bef7-44d5-bcf1-d2b721848a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_weather_api_call(\"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2a105-7ccf-450f-ac4b-4931fae5421f",
   "metadata": {},
   "source": [
    "Clearly there is a lot of information returned here. Let's refine the function to return only the temperature in Celsius in a nicely formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2983b1b-461a-42fc-9646-df03df204d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weather_api_call(location: str):\n",
    "    response = requests.get(f\"https://api.weatherapi.com/v1/current.json?key={FREE_WEATHER_API_KEY}&q={location}\")\n",
    "    if (response.status_code != 200):\n",
    "        raise ValueError(\"Could not obtain the weather for the current parameters\")\n",
    "    return f'The current weather in {location} is {response.json()[\"current\"][\"temp_c\"]} degrees celsius.'\n",
    "\n",
    "make_weather_api_call(\"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c25806-5cd5-4cf5-94e4-ea67467f2229",
   "metadata": {},
   "source": [
    "The solution is to configure an Agent to determine whether a prompt should be passed to the weather API, or to an underlying LLM for general information. We can use our previously defined LLM object to handle any non-weather related prompts.\n",
    "\n",
    "## Configuring Agents\n",
    "\n",
    "As previously mentioned, an agent is an LLM configured for making decisions on what actions to execute. Agents work with interfaces known as **tools** to interact with different functions. Our agent is expected to either make a call to an LLM, or to our weather api function - we therefore need to define two tools for the agent to use:\n",
    "\n",
    "1. A weather API tool\n",
    "2. A general-purpose LLM invocation tool\n",
    "\n",
    "We will begin by creating an Agent LLM object that is responsible for managing these tools. The model we supply must be compatible with tooling. Here we will use `llama3.1` for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ca3c4-519a-4abb-8dbe-744202136e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# We set the temperature to 0 for deterministic responses (no creative freedom).\n",
    "# We must set a model which supports tooling, see https://ollama.com/library for models tagged with \"tools\"\n",
    "agent_llm = ChatOllama(model=\"llama3.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a6abc-1a4f-4965-97d5-07ef60bfa693",
   "metadata": {},
   "source": [
    "We will now create an array of tools for the model to use. We can use langchain's tool decorator with functions to denote them as tools. We will also use `typing.Annotated` to pass descriptions for each argument. Finally, all our tools need a docstring with a description that serves as a prompt to the agent LLM to help it decide whether it is appropriate to call or not. This is a crucial point where good prompt engineering is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a5305-935f-496c-a956-026fb71dc8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool, ToolException\n",
    "from typing import Annotated\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "@tool\n",
    "def general_knowledge_tool(prompt: Annotated[str, \"The prompt to pass to the general knowledge LLM\"]) -> str:\n",
    "    \"\"\"\n",
    "    Useful when you need to answer any questions that are not related to the current real-time weather.\n",
    "    You must pass the exact input without changing anything to the function. For instance, if the input is\n",
    "\n",
    "        \"Tell me about the Americal Civil War\"\n",
    "\n",
    "    You must pass the argument\n",
    "\n",
    "        \"Tell me about the Americal Civil War\"\n",
    "\n",
    "    to the function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm.invoke(prompt).content\n",
    "    except Exception as e:\n",
    "        raise ToolException(e)\n",
    "\n",
    "@tool\n",
    "def weather_report_api(location: Annotated[str, \"The location for which to get the weather\"]) -> float:\n",
    "    \"\"\"\n",
    "    Useful when you need to answer questions related to the current real-time weather in a certain location. \n",
    "    This tool can only be used to get current real-time weather information. It does not have any information on average, or\n",
    "    historical weather at a certain location. The tool returns a single double value representing the temperature at a location\n",
    "    in degrees celsius. The only argument it takes is a string name of the location. For instance, if the question is:\n",
    "\n",
    "        \"What's the weather like in Santa Monica\"\n",
    "\n",
    "    You must pass the string \"Santa Monica\" to the tool and nothing else.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return make_weather_api_call(location)\n",
    "    except Exception as e:\n",
    "        raise ToolException(e)\n",
    "\n",
    "tools = [general_knowledge_tool, weather_report_api]\n",
    "\n",
    "# Print tool\n",
    "print(general_knowledge_tool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7625acd-98c7-465c-90ec-435cf66bd01c",
   "metadata": {},
   "source": [
    "We have defined two json tool objects above and stored them in an array. Note how the description prompt plays a huge role in the decision making for the agent.\n",
    "\n",
    "We will now bind these tools to our llm agent, and print the result of invoking it with some examples. We will check two properties of the response, namely the `tool_calls` and `content` properties (which we have used before for getting the llm response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8340466-dc57-4475-a72a-941ac2fc5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent_llm = agent_llm.bind_tools([general_knowledge_tool, weather_report_api])\n",
    "\n",
    "result = agent_llm.invoke(\"What's the current weather in Chicago?\")\n",
    "print(result.content)\n",
    "print(result.tool_calls)\n",
    "\n",
    "result = agent_llm.invoke(\"Tell me about Javascript\")\n",
    "print(result.content)\n",
    "print(result.tool_calls)\n",
    "\n",
    "# Invalid tool_calls will be listed under .invalid_tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ee2d6-5fa6-499b-88e0-95034df2fdce",
   "metadata": {},
   "source": [
    "In the printouts for `tool_calls` we can see that the agent successfully deduces which tool it needs to call, and correctly formulates the arguments such that they obey the schemas specified. Surprisingly, the `content` property is empty - it doesn't contain the outputs from the functions themselves. The agent is responsible purely for deciding what actions to take, but by itself it will not call the functions.\n",
    "\n",
    "To call the functions themselves, we can create an executor class which wraps our agent, and its tools, and calls the respective functions with their arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353a73-1d51-46f8-8658-0b6819d00b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentExecutor:\n",
    "    def __init__(self, agent, tools, verbose: bool = False):\n",
    "        self.agent = agent\n",
    "        self.tools = {tool.name: tool for tool in tools} # Create a tool map with the tool name as key\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def invoke(self, inp: str):\n",
    "        result = self.agent.invoke(inp)\n",
    "\n",
    "        if(len(result.tool_calls) == 0):\n",
    "            raise ValueError(\"Could deduce tool invocation\")\n",
    "        \n",
    "        function_name = result.tool_calls[0][\"name\"]\n",
    "        function_args = result.tool_calls[0][\"args\"]\n",
    "        try:\n",
    "            if(self.verbose):\n",
    "                print(f\"================= Calling {function_name} =================\")\n",
    "            return self.tools[function_name].invoke(input=function_args)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Could not invoke tool\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4327694-d761-43a1-b580-64251702fa0c",
   "metadata": {},
   "source": [
    "With our simple agent executor ready, let's begin using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa5055-c765-4e55-b805-4acebd583946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_executor = AgentExecutor(agent_llm, tools, verbose=True)\n",
    "\n",
    "print(agent_executor.invoke(\"What's the current weather in San Francisco?\"))\n",
    "\n",
    "print(agent_executor.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1cc8b-f147-4988-aea3-b24b4de47769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
